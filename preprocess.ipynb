{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be0ea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2\\eng\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_data_path = \"./data/English dataset/train.jsonl\"\n",
    "test_data_path = \"./data/English dataset/test.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dab40",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454c9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\t# Tokenize the text into words\n",
    "\twords = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "\t# Remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\twords = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\twords = [word for word in words if word not in stop_words]\n",
    "\n",
    "\t# Lemmatization\n",
    "\tlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "\t# Join the words back into a string\n",
    "\tpreprocessed_text = ' '.join(lemmatized_words)\n",
    "\treturn preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b42342",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=train_data_path)[\"train\"])\n",
    "test_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=test_data_path)[\"train\"])\n",
    "\n",
    "label_map = {\"Contradiction\": 1, \"Entailment\": 0, \"NotMentioned\": 0}\n",
    "train_data[\"label\"] = train_data[\"label\"].map(label_map)\n",
    "test_data[\"label\"] = test_data[\"label\"].map(label_map)\n",
    "\n",
    "train_data = train_data.drop(\"doc_id\", axis=1)\n",
    "train_data = train_data.drop(\"key\", axis=1)\n",
    "test_data = test_data.drop(\"doc_id\", axis=1)\n",
    "test_data = test_data.drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc29f7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest premise:  3098\n",
      "Longest hypothesis:  162\n",
      "---------------------------------\n",
      "mean:  296.27826449728826\n",
      "+1 std:  651.2505192635402\n",
      "+2 std:  1006.2227740297922\n",
      "+3 std:  1361.1950287960442\n"
     ]
    }
   ],
   "source": [
    "longest_premise = max(train_data['premise'].apply(len).max(), test_data['premise'].apply(len).max()) # Note: irl nebi imeli max test data, ampak bi rabil truncatat\n",
    "longest_hypotises = max(train_data['hypothesis'].apply(len).max(), test_data['hypothesis'].apply(len).max())\n",
    "longest_sentance = max(longest_premise, longest_hypotises)\n",
    "print(\"Longest premise: \", longest_premise)\n",
    "print(\"Longest hypothesis: \", longest_hypotises)\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "mean = np.mean(train_data['premise'].apply(len))\n",
    "std = np.std(train_data['premise'].apply(len))\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"+1 std: \", mean+std)\n",
    "print(\"+2 std: \", mean+2*std)\n",
    "print(\"+3 std: \", mean+3*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eadb5127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7191)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['premise'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "893c1f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4239)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data['premise'].apply(len) > 128).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db4a51",
   "metadata": {},
   "source": [
    "# Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e859aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_premise = TfidfVectorizer()\n",
    "vectorizer_hypothesis = TfidfVectorizer()\n",
    "\n",
    "train_data_vectorised = train_data.copy()\n",
    "\n",
    "X_premise= vectorizer_premise.fit_transform(train_data[\"premise\"])\n",
    "X_hypothesis = vectorizer_hypothesis.fit_transform(train_data[\"hypothesis\"])\n",
    "train_data_vectorised = hstack([X_premise, X_hypothesis])\n",
    "\n",
    "Y_premise = vectorizer_premise.transform(test_data[\"premise\"])\n",
    "Y_hypothesis = vectorizer_hypothesis.transform(test_data[\"hypothesis\"])\n",
    "test_data_vectorised = hstack([Y_premise, Y_hypothesis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "732ddec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1871\n",
      "           1       0.81      0.75      0.78       220\n",
      "\n",
      "    accuracy                           0.96      2091\n",
      "   macro avg       0.89      0.86      0.88      2091\n",
      "weighted avg       0.95      0.96      0.95      2091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "logreg_model.fit(train_data_vectorised, train_data[\"label\"])\n",
    "predictions = logreg_model.predict(test_data_vectorised)\n",
    "\n",
    "print(classification_report(test_data[\"label\"], predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "414b01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1871\n",
      "           1       0.90      0.74      0.81       220\n",
      "\n",
      "    accuracy                           0.96      2091\n",
      "   macro avg       0.93      0.86      0.89      2091\n",
      "weighted avg       0.96      0.96      0.96      2091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    random_state=67\n",
    ")\n",
    "\n",
    "rf_model.fit(train_data_vectorised, train_data[\"label\"])\n",
    "\n",
    "predictions = rf_model.predict(test_data_vectorised)\n",
    "\n",
    "print(classification_report(test_data[\"label\"], predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaf13b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_vectorised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[32m      4\u001b[39m svm_model = SVC(\n\u001b[32m      5\u001b[39m \tkernel=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m \tC=\u001b[32m1.0\u001b[39m,\n\u001b[32m      7\u001b[39m \trandom_state=\u001b[32m67\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m svm_model.fit(\u001b[43mtrain_data_vectorised\u001b[49m, train_data[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     11\u001b[39m predictions = svm_model.predict(test_data_vectorised)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(test_data[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], predictions, zero_division=\u001b[32m0\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'train_data_vectorised' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svm_model = SVC(\n",
    "\tkernel='linear',\n",
    "\tC=1.0,\n",
    "\trandom_state=67\n",
    ")\n",
    "\n",
    "svm_model.fit(train_data_vectorised, train_data[\"label\"])\n",
    "predictions = svm_model.predict(test_data_vectorised)\n",
    "\n",
    "print(classification_report(test_data[\"label\"], predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d3892",
   "metadata": {},
   "source": [
    "# Transformer-Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7661d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86dfbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "\tt =  tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=\"only_first\", stride=64, return_overflowing_tokens=True, padding='max_length', max_length=128)\n",
    "\tt[\"labels\"] = examples[\"label\"]\n",
    "\treturn t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce84564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7191/7191 [00:24<00:00, 299.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(train_data)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975d4b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 80 (from args) != 500 (from trainer_state.json)\n",
      "c:\\Users\\timna\\Desktop\\ISsem2\\eng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 2:11:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./artifacts\",\n",
    "\tlearning_rate=2e-5,\n",
    "\tper_device_train_batch_size=16,\n",
    "\tnum_train_epochs=2,\n",
    "\tweight_decay=0.01,\n",
    "\tsave_strategy=\"steps\",\n",
    "\tsave_steps=500, \n",
    "\tsave_total_limit=3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "tokenizer.save_pretrained(\"./trained_model_ex3\")\n",
    "trainer.save_model(\"./trained_model_ex3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d561dd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"./trained_model_ex3\", tokenizer=\"./trained_model_ex3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc73b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2091/2091 [00:08<00:00, 260.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "tokenized_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d47bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2\\eng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Get predicted labels\u001b[39;00m\n\u001b[32m      7\u001b[39m preds = predictions.predictions.argmax(-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(test_data[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], preds, zero_division=\u001b[32m0\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "trainer = Trainer(model=model)  # no need for args for evaluation\n",
    "predictions = trainer.predict(tokenized_dataset)\n",
    "\n",
    "# Get predicted labels\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "print(classification_report(test_data[\"label\"], preds, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
