{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_data_path = \"./data/English dataset/train.jsonl\"\n",
    "test_data_path = \"./data/English dataset/test.jsonl\"\n",
    "\n",
    "def preprocess_text(text): # From the labs\n",
    "\t# Tokenize the text into words\n",
    "\twords = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "\t# Remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\twords = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\twords = [word for word in words if word not in stop_words]\n",
    "\n",
    "\t# Lemmatization\n",
    "\tlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "\t# Join the words back into a string\n",
    "\tpreprocessed_text = ' '.join(lemmatized_words)\n",
    "\treturn preprocessed_text\n",
    "\n",
    "train_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=train_data_path)[\"train\"])\n",
    "test_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=test_data_path)[\"train\"])\n",
    "\n",
    "label_map = {\"Contradiction\": 1, \"Entailment\": 0, \"NotMentioned\": 0}\n",
    "train_data[\"label\"] = train_data[\"label\"].map(label_map)\n",
    "test_data[\"label\"] = test_data[\"label\"].map(label_map)\n",
    "\n",
    "train_data = train_data.drop(\"doc_id\", axis=1)\n",
    "train_data = train_data.drop(\"key\", axis=1)\n",
    "test_data = test_data.drop(\"doc_id\", axis=1)\n",
    "test_data = test_data.drop(\"key\", axis=1)\n",
    "\n",
    "train_data[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section was already in tims file so when you join the files you can just delete the upper preprocessing section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My adition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(train_data)\n",
    "ds = ds.select_columns([\"hypothesis\", \"premise\", \"label\"])\n",
    "ds = ds.select_columns([\"hypothesis\", \"premise\", \"label\"])\n",
    "\n",
    "dss = ds.train_test_split(0.3)\n",
    "train_dataset = dss['train']\n",
    "valid_dataset = dss['test']\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "test_corpus = test_dataset['premise']\n",
    "test_hypothesis = test_dataset['hypothesis']\n",
    "print(len(test_hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(valid_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model\n",
    "- Straight from the box, unmodified\n",
    "-  [msmarco-MiniLM-L6-cos-v5](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5) Trained specificly for query-passage retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_name = \"./models/msmarco-MiniLM-L6-cos-v5\"\n",
    "base_model = SentenceTransformer(model_name, model_kwargs={\"dtype\": \"float16\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " float16 should speed up the model, while having minimal impact on preformance: [documentation](https://www.sbert.net/docs/sentence_transformer/usage/efficiency.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the base model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " I | I i\n",
    "I I| L\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "\n",
    "fine_model = base_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup of Trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrastive loss** Used for binary labled pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import ContrastiveLoss\n",
    "\n",
    "def trainer_cl(m, train_dataset, valid_dataset):\n",
    "    td = {''}\n",
    "    loss = ContrastiveLoss(m)\n",
    "\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model = m,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        loss=loss,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if using a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_cl(fine_model, train_dataset, valid_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator, EmbeddingSimilarityEvaluator, ParaphraseMiningEvaluator, InformationRetrievalEvaluator\n",
    "\n",
    "def eval_full_inbuilt(mode, test_dataset):\n",
    "    res = []\n",
    "    res.append()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
