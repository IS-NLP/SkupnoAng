(.venv) E:\ProjectsSSD\School\IS\Idk\SkupnoAng>python hyper_search.py
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\Domen\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\Domen\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\Domen\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2026-01-02 12:54:52,167] A new study created in memory with name: no-name-9f2cba31-1938-4514-9363-19ce3f69d603
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                                                                                                                                                                | 0/405 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
{'loss': 0.0134, 'grad_norm': 0.29431235790252686, 'learning_rate': 2.0316624441685185e-05, 'epoch': 0.1}
{'loss': 0.0098, 'grad_norm': 0.2221202403306961, 'learning_rate': 1.809622832783981e-05, 'epoch': 0.2}
{'loss': 0.0077, 'grad_norm': 0.16291987895965576, 'learning_rate': 1.5875832213994433e-05, 'epoch': 0.3}
{'loss': 0.0074, 'grad_norm': 0.3999776542186737, 'learning_rate': 1.3655436100149057e-05, 'epoch': 0.4}
{'loss': 0.0072, 'grad_norm': 0.309624582529068, 'learning_rate': 1.1435039986303683e-05, 'epoch': 0.49}
{'loss': 0.0062, 'grad_norm': 0.20561012625694275, 'learning_rate': 9.214643872458307e-06, 'epoch': 0.59}
{'loss': 0.0065, 'grad_norm': 0.509391188621521, 'learning_rate': 6.994247758612931e-06, 'epoch': 0.69}
{'loss': 0.0063, 'grad_norm': 0.7544708251953125, 'learning_rate': 4.773851644767557e-06, 'epoch': 0.79}
{'loss': 0.0053, 'grad_norm': 0.21326790750026703, 'learning_rate': 2.5534555309221817e-06, 'epoch': 0.89}
{'loss': 0.006, 'grad_norm': 0.27684590220451355, 'learning_rate': 3.3305941707680624e-07, 'epoch': 0.99}
{'train_runtime': 369.8748, 'train_samples_per_second': 17.495, 'train_steps_per_second': 1.095, 'train_loss': 0.0075442984332273035, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [06:09<00:00,  1.09it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:14<00:00,  6.23it/s]
{'eval_loss': 0.005957772955298424, 'eval_sts-dev_recall@5': 0.03997648442092886, 'eval_sts-dev_recall@10': 0.10430063763397096, 'eval_sts-dev_recall@20': 0.2826301270745715, 'eval_sts-dev_normalized_recall@5': 0.13333333333333336, 'eval_sts-dev_normalized_recall@10': 0.16666666666666666, 'eval_sts-dev_normalized_recall@20': 0.29847374847374847, 'eval_sts-dev_cluster_recall': 0.3333333333333333, 'eval_runtime': 14.5323, 'eval_samples_per_second': 49.545, 'eval_steps_per_second': 6.193}
[I 2026-01-02 13:01:17,553] Trial 0 finished with value: 0.16666666666666666 and parameters: {'learning_rate': 2.126029279006947e-05, 'weight_decay': 0.09852860533039483, 'warmup_ratio': 0.05378043727137618, 'max_grad_norm': 0.9965547999538735}. Best is trial 0 with value: 0.16666666666666666.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0145, 'grad_norm': 0.3442142903804779, 'learning_rate': 8.540362198944114e-06, 'epoch': 0.1}
{'loss': 0.0112, 'grad_norm': 0.3105905055999756, 'learning_rate': 7.606989281026724e-06, 'epoch': 0.2}
{'loss': 0.009, 'grad_norm': 0.41678380966186523, 'learning_rate': 6.673616363109335e-06, 'epoch': 0.3}
{'loss': 0.0089, 'grad_norm': 0.334394246339798, 'learning_rate': 5.740243445191945e-06, 'epoch': 0.4}
{'loss': 0.0086, 'grad_norm': 0.2900993525981903, 'learning_rate': 4.8068705272745556e-06, 'epoch': 0.49}
{'loss': 0.0076, 'grad_norm': 0.3765425384044647, 'learning_rate': 3.873497609357166e-06, 'epoch': 0.59}
{'loss': 0.0086, 'grad_norm': 0.5248571038246155, 'learning_rate': 2.940124691439777e-06, 'epoch': 0.69}
{'loss': 0.0079, 'grad_norm': 0.5115283131599426, 'learning_rate': 2.006751773522387e-06, 'epoch': 0.79}
{'loss': 0.0067, 'grad_norm': 0.31382936239242554, 'learning_rate': 1.0733788556049978e-06, 'epoch': 0.89}
{'loss': 0.008, 'grad_norm': 0.4374772012233734, 'learning_rate': 1.4000593768760843e-07, 'epoch': 0.99}
{'train_runtime': 503.4493, 'train_samples_per_second': 12.853, 'train_steps_per_second': 0.804, 'train_loss': 0.009063647025161319, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:23<00:00,  1.24s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:21<00:00,  4.15it/s]
{'eval_loss': 0.006916596554219723, 'eval_sts-dev_recall@5': 0.06692443359110026, 'eval_sts-dev_recall@10': 0.21891647447203003, 'eval_sts-dev_recall@20': 0.26330212996879665, 'eval_sts-dev_normalized_recall@5': 0.17777777777777776, 'eval_sts-dev_normalized_recall@10': 0.2888888888888889, 'eval_sts-dev_normalized_recall@20': 0.2805860805860806, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 21.7299, 'eval_samples_per_second': 33.134, 'eval_steps_per_second': 4.142}
[I 2026-01-02 13:10:03,980] Trial 1 finished with value: 0.2888888888888889 and parameters: {'learning_rate': 9.053717303798678e-06, 'weight_decay': 0.16020470954563895, 'warmup_ratio': 0.04027125028295792, 'max_grad_norm': 1.431998783341151}. Best is trial 1 with value: 0.2888888888888889.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0172, 'grad_norm': 0.3954503834247589, 'learning_rate': 9.943416030029527e-07, 'epoch': 0.1}
{'loss': 0.0164, 'grad_norm': 0.4189245104789734, 'learning_rate': 2.014179144544443e-06, 'epoch': 0.2}
{'loss': 0.0121, 'grad_norm': 0.345833957195282, 'learning_rate': 2.891591098387897e-06, 'epoch': 0.3}
{'loss': 0.0124, 'grad_norm': 0.39736995100975037, 'learning_rate': 2.4871727629490307e-06, 'epoch': 0.4}
{'loss': 0.0113, 'grad_norm': 0.49665820598602295, 'learning_rate': 2.0827544275101634e-06, 'epoch': 0.49}
{'loss': 0.0105, 'grad_norm': 0.49540817737579346, 'learning_rate': 1.678336092071297e-06, 'epoch': 0.59}
{'loss': 0.0114, 'grad_norm': 0.5122042894363403, 'learning_rate': 1.2739177566324302e-06, 'epoch': 0.69}
{'loss': 0.0105, 'grad_norm': 0.4382306933403015, 'learning_rate': 8.694994211935635e-07, 'epoch': 0.79}
{'loss': 0.0094, 'grad_norm': 0.3568868041038513, 'learning_rate': 4.6508108575469677e-07, 'epoch': 0.89}
{'loss': 0.0109, 'grad_norm': 0.6517874598503113, 'learning_rate': 6.066275031583001e-08, 'epoch': 0.99}
{'train_runtime': 508.1771, 'train_samples_per_second': 12.734, 'train_steps_per_second': 0.797, 'train_loss': 0.012163411099233745, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:28<00:00,  1.25s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:22<00:00,  4.03it/s]
{'eval_loss': 0.010628197342157364, 'eval_sts-dev_recall@5': 0.036155202821869487, 'eval_sts-dev_recall@10': 0.18814724370279926, 'eval_sts-dev_recall@20': 0.34999321665988337, 'eval_sts-dev_normalized_recall@5': 0.13333333333333333, 'eval_sts-dev_normalized_recall@10': 0.25555555555555554, 'eval_sts-dev_normalized_recall@20': 0.36727716727716725, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 22.401, 'eval_samples_per_second': 32.141, 'eval_steps_per_second': 4.018}
[I 2026-01-02 13:18:55,706] Trial 2 finished with value: 0.25555555555555554 and parameters: {'learning_rate': 2.932032931931784e-06, 'weight_decay': 0.15939433746102508, 'warmup_ratio': 0.2832003353607505, 'max_grad_norm': 0.9365746679131358}. Best is trial 1 with value: 0.2888888888888889.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0159, 'grad_norm': 0.40348929166793823, 'learning_rate': 5.747091674951649e-06, 'epoch': 0.1}
{'loss': 0.0128, 'grad_norm': 0.3704119324684143, 'learning_rate': 7.8685866124338e-06, 'epoch': 0.2}
{'loss': 0.0095, 'grad_norm': 0.36070117354393005, 'learning_rate': 6.9031158624419235e-06, 'epoch': 0.3}
{'loss': 0.0092, 'grad_norm': 0.5824183225631714, 'learning_rate': 5.937645112450045e-06, 'epoch': 0.4}
{'loss': 0.0091, 'grad_norm': 0.3032611906528473, 'learning_rate': 4.972174362458168e-06, 'epoch': 0.49}
{'loss': 0.0079, 'grad_norm': 0.40604981780052185, 'learning_rate': 4.006703612466291e-06, 'epoch': 0.59}
{'loss': 0.0088, 'grad_norm': 0.6310397386550903, 'learning_rate': 3.0412328624744136e-06, 'epoch': 0.69}
{'loss': 0.0082, 'grad_norm': 0.49654528498649597, 'learning_rate': 2.0757621124825363e-06, 'epoch': 0.79}
{'loss': 0.007, 'grad_norm': 0.30074548721313477, 'learning_rate': 1.1102913624906589e-06, 'epoch': 0.89}
{'loss': 0.0079, 'grad_norm': 0.5148957967758179, 'learning_rate': 1.448206124987816e-07, 'epoch': 0.99}
{'train_runtime': 503.2523, 'train_samples_per_second': 12.858, 'train_steps_per_second': 0.805, 'train_loss': 0.009596720806978367, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:23<00:00,  1.24s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:20<00:00,  4.48it/s]
{'eval_loss': 0.007593645714223385, 'eval_sts-dev_recall@5': 0.06631393298059965, 'eval_sts-dev_recall@10': 0.21891647447203003, 'eval_sts-dev_recall@20': 0.27408311852756295, 'eval_sts-dev_normalized_recall@5': 0.17777777777777776, 'eval_sts-dev_normalized_recall@10': 0.2888888888888889, 'eval_sts-dev_normalized_recall@20': 0.28992673992673995, 'eval_sts-dev_cluster_recall': 0.3333333333333333, 'eval_runtime': 20.1246, 'eval_samples_per_second': 35.777, 'eval_steps_per_second': 4.472}
[I 2026-01-02 13:27:40,114] Trial 3 finished with value: 0.2888888888888889 and parameters: {'learning_rate': 8.399595524929333e-06, 'weight_decay': 0.07166455027220424, 'warmup_ratio': 0.13951491464324578, 'max_grad_norm': 1.724330898558081}. Best is trial 1 with value: 0.2888888888888889.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0144, 'grad_norm': 0.33127450942993164, 'learning_rate': 1.173438131511063e-05, 'epoch': 0.1}
{'loss': 0.0106, 'grad_norm': 0.2519545555114746, 'learning_rate': 1.0451935269743347e-05, 'epoch': 0.2}
{'loss': 0.0088, 'grad_norm': 0.37499916553497314, 'learning_rate': 9.169489224376066e-06, 'epoch': 0.3}
{'loss': 0.0084, 'grad_norm': 0.30505821108818054, 'learning_rate': 7.887043179008784e-06, 'epoch': 0.4}
{'loss': 0.008, 'grad_norm': 0.33261916041374207, 'learning_rate': 6.604597133641501e-06, 'epoch': 0.49}
{'loss': 0.0068, 'grad_norm': 0.28262820839881897, 'learning_rate': 5.32215108827422e-06, 'epoch': 0.59}
{'loss': 0.0077, 'grad_norm': 0.6620208621025085, 'learning_rate': 4.039705042906938e-06, 'epoch': 0.69}
{'loss': 0.0072, 'grad_norm': 0.4051744341850281, 'learning_rate': 2.7572589975396558e-06, 'epoch': 0.79}
{'loss': 0.0061, 'grad_norm': 0.19574490189552307, 'learning_rate': 1.4748129521723742e-06, 'epoch': 0.89}
{'loss': 0.0072, 'grad_norm': 0.6014147996902466, 'learning_rate': 1.9236690680509226e-07, 'epoch': 0.99}
{'train_runtime': 497.5392, 'train_samples_per_second': 13.006, 'train_steps_per_second': 0.814, 'train_loss': 0.008476939898582152, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:17<00:00,  1.23s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:23<00:00,  3.82it/s]
{'eval_loss': 0.006499427370727062, 'eval_sts-dev_recall@5': 0.1859720526387193, 'eval_sts-dev_recall@10': 0.2570117125672681, 'eval_sts-dev_recall@20': 0.28645140867363084, 'eval_sts-dev_normalized_recall@5': 0.3111111111111111, 'eval_sts-dev_normalized_recall@10': 0.33333333333333326, 'eval_sts-dev_normalized_recall@20': 0.30085470085470084, 'eval_sts-dev_cluster_recall': 0.4444444444444444, 'eval_runtime': 23.6135, 'eval_samples_per_second': 30.491, 'eval_steps_per_second': 3.811}
[I 2026-01-02 13:36:22,471] Trial 4 finished with value: 0.33333333333333326 and parameters: {'learning_rate': 1.2247359733257542e-05, 'weight_decay': 0.09092585204374326, 'warmup_ratio': 0.05503071687326718, 'max_grad_norm': 0.8774817671930895}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0168, 'grad_norm': 0.39957761764526367, 'learning_rate': 2.1365928620323763e-06, 'epoch': 0.1}
{'loss': 0.0151, 'grad_norm': 0.4027017056941986, 'learning_rate': 4.327970156424557e-06, 'epoch': 0.2}
{'loss': 0.0108, 'grad_norm': 0.32898786664009094, 'learning_rate': 4.735770673829533e-06, 'epoch': 0.3}
{'loss': 0.0109, 'grad_norm': 0.3717339336872101, 'learning_rate': 4.073425125042185e-06, 'epoch': 0.4}
{'loss': 0.01, 'grad_norm': 0.42544546723365784, 'learning_rate': 3.4110795762548386e-06, 'epoch': 0.49}
{'loss': 0.009, 'grad_norm': 0.5668437480926514, 'learning_rate': 2.748734027467491e-06, 'epoch': 0.59}
{'loss': 0.0099, 'grad_norm': 0.5891802906990051, 'learning_rate': 2.0863884786801437e-06, 'epoch': 0.69}
{'loss': 0.0091, 'grad_norm': 0.5752500891685486, 'learning_rate': 1.4240429298927967e-06, 'epoch': 0.79}
{'loss': 0.008, 'grad_norm': 0.29531699419021606, 'learning_rate': 7.616973811054493e-07, 'epoch': 0.89}
{'loss': 0.0093, 'grad_norm': 0.563890278339386, 'learning_rate': 9.93518323181021e-08, 'epoch': 0.99}
{'train_runtime': 566.439, 'train_samples_per_second': 11.424, 'train_steps_per_second': 0.715, 'train_loss': 0.010833358691062457, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [09:26<00:00,  1.40s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:25<00:00,  3.48it/s]
{'eval_loss': 0.008537743240594864, 'eval_sts-dev_recall@5': 0.03233392122281011, 'eval_sts-dev_recall@10': 0.18814724370279926, 'eval_sts-dev_recall@20': 0.2388821055487722, 'eval_sts-dev_normalized_recall@5': 0.13333333333333336, 'eval_sts-dev_normalized_recall@10': 0.25555555555555554, 'eval_sts-dev_normalized_recall@20': 0.25616605616605614, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 25.926, 'eval_samples_per_second': 27.771, 'eval_steps_per_second': 3.471}
[I 2026-01-02 13:46:16,023] Trial 5 finished with value: 0.25555555555555554 and parameters: {'learning_rate': 5.149736641821625e-06, 'weight_decay': 0.06975046703253887, 'warmup_ratio': 0.23033389406774116, 'max_grad_norm': 1.4809057523727254}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0168, 'grad_norm': 0.40222033858299255, 'learning_rate': 2.198117367372007e-06, 'epoch': 0.1}
{'loss': 0.015, 'grad_norm': 0.3939776122570038, 'learning_rate': 4.452596718522783e-06, 'epoch': 0.2}
{'loss': 0.0108, 'grad_norm': 0.32911938428878784, 'learning_rate': 4.93985515893279e-06, 'epoch': 0.3}
{'loss': 0.0108, 'grad_norm': 0.42557790875434875, 'learning_rate': 4.248966325515616e-06, 'epoch': 0.4}
{'loss': 0.0099, 'grad_norm': 0.4412206709384918, 'learning_rate': 3.558077492098443e-06, 'epoch': 0.49}
{'loss': 0.0089, 'grad_norm': 0.5429528951644897, 'learning_rate': 2.86718865868127e-06, 'epoch': 0.59}
{'loss': 0.0098, 'grad_norm': 0.557735800743103, 'learning_rate': 2.176299825264096e-06, 'epoch': 0.69}
{'loss': 0.0091, 'grad_norm': 0.49133625626564026, 'learning_rate': 1.4854109918469228e-06, 'epoch': 0.79}
{'loss': 0.008, 'grad_norm': 0.3244256377220154, 'learning_rate': 7.945221584297494e-07, 'epoch': 0.89}
{'loss': 0.0092, 'grad_norm': 0.6360284090042114, 'learning_rate': 1.0363332501257602e-07, 'epoch': 0.99}
{'train_runtime': 589.8624, 'train_samples_per_second': 10.97, 'train_steps_per_second': 0.687, 'train_loss': 0.010777673152861772, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [09:49<00:00,  1.46s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:37<00:00,  2.42it/s]
{'eval_loss': 0.008449451066553593, 'eval_sts-dev_recall@5': 0.14344503233392122, 'eval_sts-dev_recall@10': 0.18814724370279926, 'eval_sts-dev_recall@20': 0.24681861348528017, 'eval_sts-dev_normalized_recall@5': 0.24444444444444446, 'eval_sts-dev_normalized_recall@10': 0.25555555555555554, 'eval_sts-dev_normalized_recall@20': 0.2641025641025641, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 37.1485, 'eval_samples_per_second': 19.382, 'eval_steps_per_second': 2.423}
[I 2026-01-02 13:56:44,477] Trial 6 finished with value: 0.25555555555555554 and parameters: {'learning_rate': 5.354388458983094e-06, 'weight_decay': 0.15387607481153454, 'warmup_ratio': 0.23377593289523382, 'max_grad_norm': 0.5609709964309939}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0174, 'grad_norm': 0.4024340808391571, 'learning_rate': 5.323090018301013e-07, 'epoch': 0.1}
{'loss': 0.0172, 'grad_norm': 0.422357976436615, 'learning_rate': 1.078266952425077e-06, 'epoch': 0.2}
{'loss': 0.0131, 'grad_norm': 0.36390137672424316, 'learning_rate': 1.5479790512990301e-06, 'epoch': 0.3}
{'loss': 0.0136, 'grad_norm': 0.46913617849349976, 'learning_rate': 1.3314784846837813e-06, 'epoch': 0.4}
{'loss': 0.0128, 'grad_norm': 0.46403416991233826, 'learning_rate': 1.1149779180685322e-06, 'epoch': 0.49}
{'loss': 0.0117, 'grad_norm': 0.5231506824493408, 'learning_rate': 8.984773514532833e-07, 'epoch': 0.59}
{'loss': 0.0128, 'grad_norm': 0.8786407113075256, 'learning_rate': 6.819767848380342e-07, 'epoch': 0.69}
{'loss': 0.0117, 'grad_norm': 0.4443072974681854, 'learning_rate': 4.654762182227853e-07, 'epoch': 0.79}
{'loss': 0.0106, 'grad_norm': 0.33703741431236267, 'learning_rate': 2.4897565160753634e-07, 'epoch': 0.89}
{'loss': 0.0126, 'grad_norm': 0.650843620300293, 'learning_rate': 3.2475084992287343e-08, 'epoch': 0.99}
{'train_runtime': 566.6274, 'train_samples_per_second': 11.42, 'train_steps_per_second': 0.715, 'train_loss': 0.013319907263841158, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [09:26<00:00,  1.40s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:38<00:00,  2.32it/s]
{'eval_loss': 0.012825707904994488, 'eval_sts-dev_recall@5': 0.015873015873015872, 'eval_sts-dev_recall@10': 0.1596346040790485, 'eval_sts-dev_recall@20': 0.19640030751141863, 'eval_sts-dev_normalized_recall@5': 0.044444444444444446, 'eval_sts-dev_normalized_recall@10': 0.18888888888888888, 'eval_sts-dev_normalized_recall@20': 0.2036019536019536, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 39.0841, 'eval_samples_per_second': 18.422, 'eval_steps_per_second': 2.303}
[I 2026-01-02 14:06:51,354] Trial 7 finished with value: 0.18888888888888888 and parameters: {'learning_rate': 1.569629107960555e-06, 'weight_decay': 0.08451045117697067, 'warmup_ratio': 0.2824481592749447, 'max_grad_norm': 1.1164112160906876}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0149, 'grad_norm': 0.3685930073261261, 'learning_rate': 9.9257186854296e-06, 'epoch': 0.1}
{'loss': 0.0112, 'grad_norm': 0.3091890513896942, 'learning_rate': 8.840940687021993e-06, 'epoch': 0.2}
{'loss': 0.0089, 'grad_norm': 0.5052008628845215, 'learning_rate': 7.756162688614386e-06, 'epoch': 0.3}
{'loss': 0.0088, 'grad_norm': 0.38631945848464966, 'learning_rate': 6.6713846902067806e-06, 'epoch': 0.4}
{'loss': 0.0084, 'grad_norm': 0.35085877776145935, 'learning_rate': 5.586606691799174e-06, 'epoch': 0.49}
{'loss': 0.0073, 'grad_norm': 0.3410624563694, 'learning_rate': 4.501828693391567e-06, 'epoch': 0.59}
{'loss': 0.0084, 'grad_norm': 0.6142444610595703, 'learning_rate': 3.4170506949839604e-06, 'epoch': 0.69}
{'loss': 0.0077, 'grad_norm': 0.5047175288200378, 'learning_rate': 2.332272696576354e-06, 'epoch': 0.79}
{'loss': 0.0065, 'grad_norm': 0.27511507272720337, 'learning_rate': 1.2474946981687475e-06, 'epoch': 0.89}
{'loss': 0.0075, 'grad_norm': 0.3894186317920685, 'learning_rate': 1.6271669976114098e-07, 'epoch': 0.99}
{'train_runtime': 539.9201, 'train_samples_per_second': 11.985, 'train_steps_per_second': 0.75, 'train_loss': 0.008920651908825945, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:59<00:00,  1.33s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:22<00:00,  3.98it/s]
{'eval_loss': 0.006861204747110605, 'eval_sts-dev_recall@5': 0.0748609415276082, 'eval_sts-dev_recall@10': 0.20213901325012434, 'eval_sts-dev_recall@20': 0.25125039569484015, 'eval_sts-dev_normalized_recall@5': 0.2, 'eval_sts-dev_normalized_recall@10': 0.2555555555555556, 'eval_sts-dev_normalized_recall@20': 0.26709401709401714, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 22.6075, 'eval_samples_per_second': 31.848, 'eval_steps_per_second': 3.981}
[I 2026-01-02 14:16:14,949] Trial 8 finished with value: 0.2555555555555556 and parameters: {'learning_rate': 1.0224032634991691e-05, 'weight_decay': 0.09715798976763133, 'warmup_ratio': 0.06698670260751818, 'max_grad_norm': 1.6821142478441118}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0148, 'grad_norm': 0.35381463170051575, 'learning_rate': 8.544141542464945e-06, 'epoch': 0.1}
{'loss': 0.0115, 'grad_norm': 0.32797136902809143, 'learning_rate': 7.610355581539814e-06, 'epoch': 0.2}
{'loss': 0.0092, 'grad_norm': 0.43095532059669495, 'learning_rate': 6.676569620614683e-06, 'epoch': 0.3}
{'loss': 0.009, 'grad_norm': 0.37569835782051086, 'learning_rate': 5.742783659689553e-06, 'epoch': 0.4}
{'loss': 0.0087, 'grad_norm': 0.3105844259262085, 'learning_rate': 4.808997698764423e-06, 'epoch': 0.49}
{'loss': 0.0076, 'grad_norm': 0.4148007333278656, 'learning_rate': 3.875211737839292e-06, 'epoch': 0.59}
{'loss': 0.0086, 'grad_norm': 0.5395814180374146, 'learning_rate': 2.9414257769141614e-06, 'epoch': 0.69}
{'loss': 0.0079, 'grad_norm': 0.4830293357372284, 'learning_rate': 2.0076398159890306e-06, 'epoch': 0.79}
{'loss': 0.0067, 'grad_norm': 0.20504559576511383, 'learning_rate': 1.0738538550639002e-06, 'epoch': 0.89}
{'loss': 0.0079, 'grad_norm': 0.38389497995376587, 'learning_rate': 1.4006789413876959e-07, 'epoch': 0.99}
{'train_runtime': 585.0762, 'train_samples_per_second': 11.06, 'train_steps_per_second': 0.692, 'train_loss': 0.00915234920879205, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [09:45<00:00,  1.44s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:27<00:00,  3.30it/s]
{'eval_loss': 0.007076837122440338, 'eval_sts-dev_recall@5': 0.19008727897616784, 'eval_sts-dev_recall@10': 0.21830597386152942, 'eval_sts-dev_recall@20': 0.24681861348528017, 'eval_sts-dev_normalized_recall@5': 0.3333333333333333, 'eval_sts-dev_normalized_recall@10': 0.2888888888888889, 'eval_sts-dev_normalized_recall@20': 0.2641025641025641, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 27.2768, 'eval_samples_per_second': 26.396, 'eval_steps_per_second': 3.3}
[I 2026-01-02 14:26:28,347] Trial 9 finished with value: 0.2888888888888889 and parameters: {'learning_rate': 8.917655926834997e-06, 'weight_decay': 0.1048710869508791, 'warmup_ratio': 0.0559998648641691, 'max_grad_norm': 1.694514161701089}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0144, 'grad_norm': 0.33012476563453674, 'learning_rate': 1.9796854273660903e-05, 'epoch': 0.1}
{'loss': 0.0097, 'grad_norm': 0.20811378955841064, 'learning_rate': 2.4916233110454046e-05, 'epoch': 0.2}
{'loss': 0.0075, 'grad_norm': 0.2589360773563385, 'learning_rate': 2.1859026593833918e-05, 'epoch': 0.3}
{'loss': 0.0074, 'grad_norm': 0.29572221636772156, 'learning_rate': 1.8801820077213787e-05, 'epoch': 0.4}
{'loss': 0.0069, 'grad_norm': 0.233024001121521, 'learning_rate': 1.574461356059366e-05, 'epoch': 0.49}
{'loss': 0.0055, 'grad_norm': 0.26838213205337524, 'learning_rate': 1.2687407043973533e-05, 'epoch': 0.59}
{'loss': 0.0065, 'grad_norm': 0.9567639827728271, 'learning_rate': 9.630200527353404e-06, 'epoch': 0.69}
{'loss': 0.0059, 'grad_norm': 0.9255436062812805, 'learning_rate': 6.572994010733276e-06, 'epoch': 0.79}
{'loss': 0.0056, 'grad_norm': 0.1954670548439026, 'learning_rate': 3.5157874941131475e-06, 'epoch': 0.89}
{'loss': 0.006, 'grad_norm': 0.49609941244125366, 'learning_rate': 4.585809774930192e-07, 'epoch': 0.99}
{'train_runtime': 532.8347, 'train_samples_per_second': 12.144, 'train_steps_per_second': 0.76, 'train_loss': 0.007510009950693743, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:52<00:00,  1.32s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:23<00:00,  3.84it/s]
{'eval_loss': 0.0067068953067064285, 'eval_sts-dev_recall@5': 0.06663048885271107, 'eval_sts-dev_recall@10': 0.23261429928096597, 'eval_sts-dev_recall@20': 0.25732826843937956, 'eval_sts-dev_normalized_recall@5': 0.15555555555555556, 'eval_sts-dev_normalized_recall@10': 0.2777777777777778, 'eval_sts-dev_normalized_recall@20': 0.26452991452991453, 'eval_sts-dev_cluster_recall': 0.4444444444444444, 'eval_runtime': 23.4535, 'eval_samples_per_second': 30.699, 'eval_steps_per_second': 3.837}
[I 2026-01-02 14:35:45,688] Trial 10 finished with value: 0.2777777777777778 and parameters: {'learning_rate': 2.690341734625713e-05, 'weight_decay': 0.0019265794991854684, 'warmup_ratio': 0.12907476631817472, 'max_grad_norm': 0.6496081861162462}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0129, 'grad_norm': 0.3999636173248291, 'learning_rate': 1.4189228334098366e-05, 'epoch': 0.1}
{'loss': 0.0099, 'grad_norm': 0.29768118262290955, 'learning_rate': 1.2638492997038434e-05, 'epoch': 0.2}
{'loss': 0.0082, 'grad_norm': 0.25918906927108765, 'learning_rate': 1.1087757659978504e-05, 'epoch': 0.3}
{'loss': 0.0081, 'grad_norm': 0.4453144967556, 'learning_rate': 9.537022322918574e-06, 'epoch': 0.4}
{'loss': 0.0074, 'grad_norm': 0.2737056612968445, 'learning_rate': 7.986286985858644e-06, 'epoch': 0.49}
{'loss': 0.0064, 'grad_norm': 0.288085401058197, 'learning_rate': 6.435551648798712e-06, 'epoch': 0.59}
{'loss': 0.0071, 'grad_norm': 0.5189722180366516, 'learning_rate': 4.884816311738782e-06, 'epoch': 0.69}
{'loss': 0.0065, 'grad_norm': 0.3364163935184479, 'learning_rate': 3.3340809746788514e-06, 'epoch': 0.79}
{'loss': 0.0059, 'grad_norm': 0.24204346537590027, 'learning_rate': 1.7833456376189204e-06, 'epoch': 0.89}
{'loss': 0.0065, 'grad_norm': 0.42828187346458435, 'learning_rate': 2.326103005589896e-07, 'epoch': 0.99}
{'train_runtime': 488.3707, 'train_samples_per_second': 13.25, 'train_steps_per_second': 0.829, 'train_loss': 0.007839557891826571, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:08<00:00,  1.21s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:21<00:00,  4.26it/s]
{'eval_loss': 0.006094206124544144, 'eval_sts-dev_recall@5': 0.05898792565459232, 'eval_sts-dev_recall@10': 0.11707140596029486, 'eval_sts-dev_recall@20': 0.27256817256817256, 'eval_sts-dev_normalized_recall@5': 0.15555555555555556, 'eval_sts-dev_normalized_recall@10': 0.16666666666666666, 'eval_sts-dev_normalized_recall@20': 0.28553113553113557, 'eval_sts-dev_cluster_recall': 0.3333333333333333, 'eval_runtime': 21.1955, 'eval_samples_per_second': 33.97, 'eval_steps_per_second': 4.246}
[I 2026-01-02 14:44:16,287] Trial 11 finished with value: 0.16666666666666666 and parameters: {'learning_rate': 1.5391048220319812e-05, 'weight_decay': 0.19570882208138474, 'warmup_ratio': 0.017308575393726457, 'max_grad_norm': 0.7752278192678338}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0131, 'grad_norm': 0.3391554057598114, 'learning_rate': 1.2700181374899405e-05, 'epoch': 0.1}
{'loss': 0.0106, 'grad_norm': 0.37633147835731506, 'learning_rate': 1.1312183410429527e-05, 'epoch': 0.2}
{'loss': 0.009, 'grad_norm': 0.45456627011299133, 'learning_rate': 9.924185445959645e-06, 'epoch': 0.3}
{'loss': 0.0084, 'grad_norm': 0.2800756096839905, 'learning_rate': 8.536187481489764e-06, 'epoch': 0.4}
{'loss': 0.0076, 'grad_norm': 0.23772405087947845, 'learning_rate': 7.1481895170198845e-06, 'epoch': 0.49}
{'loss': 0.0065, 'grad_norm': 0.270526647567749, 'learning_rate': 5.760191552550003e-06, 'epoch': 0.59}
{'loss': 0.0075, 'grad_norm': 0.576472282409668, 'learning_rate': 4.372193588080123e-06, 'epoch': 0.69}
{'loss': 0.0068, 'grad_norm': 0.5489013195037842, 'learning_rate': 2.984195623610243e-06, 'epoch': 0.79}
{'loss': 0.0059, 'grad_norm': 0.20691493153572083, 'learning_rate': 1.5961976591403624e-06, 'epoch': 0.89}
{'loss': 0.0071, 'grad_norm': 0.4935855567455292, 'learning_rate': 2.0819969467048206e-07, 'epoch': 0.99}
{'train_runtime': 533.3703, 'train_samples_per_second': 12.132, 'train_steps_per_second': 0.759, 'train_loss': 0.008211894066613398, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:53<00:00,  1.32s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:21<00:00,  4.18it/s]
{'eval_loss': 0.006272762548178434, 'eval_sts-dev_recall@5': 0.05044091710758378, 'eval_sts-dev_recall@10': 0.1935920047031158, 'eval_sts-dev_recall@20': 0.26402116402116405, 'eval_sts-dev_normalized_recall@5': 0.13333333333333333, 'eval_sts-dev_normalized_recall@10': 0.24444444444444446, 'eval_sts-dev_normalized_recall@20': 0.276984126984127, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 21.6069, 'eval_samples_per_second': 33.323, 'eval_steps_per_second': 4.165}
[I 2026-01-02 14:53:32,447] Trial 12 finished with value: 0.24444444444444446 and parameters: {'learning_rate': 1.4018779441145792e-05, 'weight_decay': 0.1338060930450246, 'warmup_ratio': 0.0008594820325185035, 'max_grad_norm': 1.260009264938025}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0164, 'grad_norm': 0.39994192123413086, 'learning_rate': 3.443343456757255e-06, 'epoch': 0.1}
{'loss': 0.0144, 'grad_norm': 0.3673255741596222, 'learning_rate': 3.242020896239608e-06, 'epoch': 0.2}
{'loss': 0.0108, 'grad_norm': 0.3262295722961426, 'learning_rate': 2.844226921240883e-06, 'epoch': 0.3}
{'loss': 0.0111, 'grad_norm': 0.4252495765686035, 'learning_rate': 2.446432946242158e-06, 'epoch': 0.4}
{'loss': 0.0105, 'grad_norm': 0.6130667328834534, 'learning_rate': 2.0486389712434332e-06, 'epoch': 0.49}
{'loss': 0.0098, 'grad_norm': 0.4829723834991455, 'learning_rate': 1.6508449962447081e-06, 'epoch': 0.59}
{'loss': 0.0107, 'grad_norm': 0.4833061993122101, 'learning_rate': 1.2530510212459832e-06, 'epoch': 0.69}
{'loss': 0.01, 'grad_norm': 0.48543781042099, 'learning_rate': 8.552570462472585e-07, 'epoch': 0.79}
{'loss': 0.0088, 'grad_norm': 0.32361099123954773, 'learning_rate': 4.574630712485336e-07, 'epoch': 0.89}
{'loss': 0.0102, 'grad_norm': 0.5880865454673767, 'learning_rate': 5.966909624980874e-08, 'epoch': 0.99}
{'train_runtime': 440.5097, 'train_samples_per_second': 14.69, 'train_steps_per_second': 0.919, 'train_loss': 0.011240096647798278, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [07:20<00:00,  1.09s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:18<00:00,  4.96it/s]
{'eval_loss': 0.010139422491192818, 'eval_sts-dev_recall@5': 0.05263871930538597, 'eval_sts-dev_recall@10': 0.20213901325012434, 'eval_sts-dev_recall@20': 0.2227151449373672, 'eval_sts-dev_normalized_recall@5': 0.17777777777777778, 'eval_sts-dev_normalized_recall@10': 0.2555555555555556, 'eval_sts-dev_normalized_recall@20': 0.2371184371184371, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 18.2435, 'eval_samples_per_second': 39.466, 'eval_steps_per_second': 4.933}
[I 2026-01-02 15:01:12,228] Trial 13 finished with value: 0.2555555555555556 and parameters: {'learning_rate': 3.6199251724883963e-06, 'weight_decay': 0.02293372970512629, 'warmup_ratio': 0.1003715316636366, 'max_grad_norm': 0.7987509361606553}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0156, 'grad_norm': 0.42398887872695923, 'learning_rate': 7.719383403407972e-06, 'epoch': 0.1}
{'loss': 0.012, 'grad_norm': 0.3538011610507965, 'learning_rate': 1.4187973913678011e-05, 'epoch': 0.2}
{'loss': 0.0088, 'grad_norm': 0.3241395056247711, 'learning_rate': 1.2447118218748196e-05, 'epoch': 0.3}
{'loss': 0.0085, 'grad_norm': 0.4163050651550293, 'learning_rate': 1.0706262523818377e-05, 'epoch': 0.4}
{'loss': 0.0079, 'grad_norm': 0.25271105766296387, 'learning_rate': 8.96540682888856e-06, 'epoch': 0.49}
{'loss': 0.0068, 'grad_norm': 0.37021809816360474, 'learning_rate': 7.224551133958742e-06, 'epoch': 0.59}
{'loss': 0.0075, 'grad_norm': 0.8451036810874939, 'learning_rate': 5.483695439028925e-06, 'epoch': 0.69}
{'loss': 0.007, 'grad_norm': 0.5894718766212463, 'learning_rate': 3.7428397440991072e-06, 'epoch': 0.79}
{'loss': 0.0061, 'grad_norm': 0.2504851520061493, 'learning_rate': 2.00198404916929e-06, 'epoch': 0.89}
{'loss': 0.0066, 'grad_norm': 0.3743349015712738, 'learning_rate': 2.611283542394726e-07, 'epoch': 0.99}
{'train_runtime': 546.7492, 'train_samples_per_second': 11.835, 'train_steps_per_second': 0.741, 'train_loss': 0.008626682918380807, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [09:06<00:00,  1.35s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:32<00:00,  2.74it/s]
{'eval_loss': 0.006276438012719154, 'eval_sts-dev_recall@5': 0.06280920725365169, 'eval_sts-dev_recall@10': 0.09896441007552118, 'eval_sts-dev_recall@20': 0.18800253244697687, 'eval_sts-dev_normalized_recall@5': 0.15555555555555556, 'eval_sts-dev_normalized_recall@10': 0.15555555555555556, 'eval_sts-dev_normalized_recall@20': 0.20384615384615387, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 32.9442, 'eval_samples_per_second': 21.855, 'eval_steps_per_second': 2.732}
[I 2026-01-02 15:10:53,103] Trial 14 finished with value: 0.15555555555555556 and parameters: {'learning_rate': 1.4449102267917484e-05, 'weight_decay': 0.18914549063596933, 'warmup_ratio': 0.17847546191128166, 'max_grad_norm': 1.2373316522966233}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0155, 'grad_norm': 0.40157872438430786, 'learning_rate': 7.775001192163118e-06, 'epoch': 0.1}
{'loss': 0.0123, 'grad_norm': 0.3485671281814575, 'learning_rate': 6.925274285915783e-06, 'epoch': 0.2}
{'loss': 0.0095, 'grad_norm': 0.4013175666332245, 'learning_rate': 6.075547379668447e-06, 'epoch': 0.3}
{'loss': 0.0095, 'grad_norm': 0.48071011900901794, 'learning_rate': 5.225820473421113e-06, 'epoch': 0.4}
{'loss': 0.0089, 'grad_norm': 0.29647496342658997, 'learning_rate': 4.376093567173777e-06, 'epoch': 0.49}
{'loss': 0.008, 'grad_norm': 0.4388147294521332, 'learning_rate': 3.526366660926442e-06, 'epoch': 0.59}
{'loss': 0.0089, 'grad_norm': 0.5519651770591736, 'learning_rate': 2.6766397546791065e-06, 'epoch': 0.69}
{'loss': 0.0083, 'grad_norm': 0.6231575608253479, 'learning_rate': 1.8269128484317709e-06, 'epoch': 0.79}
{'loss': 0.007, 'grad_norm': 0.2299187034368515, 'learning_rate': 9.771859421844355e-07, 'epoch': 0.89}
{'loss': 0.0082, 'grad_norm': 0.4856584668159485, 'learning_rate': 1.274590359371003e-07, 'epoch': 0.99}
{'train_runtime': 467.6153, 'train_samples_per_second': 13.838, 'train_steps_per_second': 0.866, 'train_loss': 0.009582360981055247, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [07:47<00:00,  1.15s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:17<00:00,  5.15it/s]
{'eval_loss': 0.007538292091339827, 'eval_sts-dev_recall@5': 0.07074571519015964, 'eval_sts-dev_recall@10': 0.2062542395875729, 'eval_sts-dev_recall@20': 0.25186089630534075, 'eval_sts-dev_normalized_recall@5': 0.17777777777777778, 'eval_sts-dev_normalized_recall@10': 0.2666666666666667, 'eval_sts-dev_normalized_recall@20': 0.26770451770451775, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 17.5044, 'eval_samples_per_second': 41.132, 'eval_steps_per_second': 5.142}
[I 2026-01-02 15:18:59,212] Trial 15 finished with value: 0.2666666666666667 and parameters: {'learning_rate': 7.796244364819302e-06, 'weight_decay': 0.13139605849917133, 'warmup_ratio': 0.09356942975048317, 'max_grad_norm': 0.8325859785004347}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0158, 'grad_norm': 0.39185863733291626, 'learning_rate': 2.675092450567585e-06, 'epoch': 0.1}
{'loss': 0.0143, 'grad_norm': 0.38721662759780884, 'learning_rate': 2.382732619904461e-06, 'epoch': 0.2}
{'loss': 0.011, 'grad_norm': 0.36246615648269653, 'learning_rate': 2.090372789241337e-06, 'epoch': 0.3}
{'loss': 0.0116, 'grad_norm': 0.45262017846107483, 'learning_rate': 1.798012958578213e-06, 'epoch': 0.4}
{'loss': 0.011, 'grad_norm': 0.47005796432495117, 'learning_rate': 1.505653127915089e-06, 'epoch': 0.49}
{'loss': 0.0103, 'grad_norm': 0.5137445330619812, 'learning_rate': 1.2132932972519648e-06, 'epoch': 0.59}
{'loss': 0.0111, 'grad_norm': 0.5701672434806824, 'learning_rate': 9.209334665888407e-07, 'epoch': 0.69}
{'loss': 0.0106, 'grad_norm': 0.5157539248466492, 'learning_rate': 6.285736359257168e-07, 'epoch': 0.79}
{'loss': 0.0094, 'grad_norm': 0.37097305059432983, 'learning_rate': 3.3621380526259266e-07, 'epoch': 0.89}
{'loss': 0.011, 'grad_norm': 0.6366205215454102, 'learning_rate': 4.3853974599468605e-08, 'epoch': 0.99}
{'train_runtime': 513.0921, 'train_samples_per_second': 12.612, 'train_steps_per_second': 0.789, 'train_loss': 0.011565263357795315, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:33<00:00,  1.27s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:21<00:00,  4.12it/s]
{'eval_loss': 0.010960749350488186, 'eval_sts-dev_recall@5': 0.027924750146972363, 'eval_sts-dev_recall@10': 0.2062542395875729, 'eval_sts-dev_recall@20': 0.3338262560484783, 'eval_sts-dev_normalized_recall@5': 0.08888888888888889, 'eval_sts-dev_normalized_recall@10': 0.2666666666666667, 'eval_sts-dev_normalized_recall@20': 0.34822954822954827, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 21.8543, 'eval_samples_per_second': 32.945, 'eval_steps_per_second': 4.118}
[I 2026-01-02 15:27:55,137] Trial 16 finished with value: 0.2666666666666667 and parameters: {'learning_rate': 2.872435336265194e-06, 'weight_decay': 0.04818020126876707, 'warmup_ratio': 0.028935424874125325, 'max_grad_norm': 1.9997845781452874}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0152, 'grad_norm': 0.3601980209350586, 'learning_rate': 1.0840950552114422e-05, 'epoch': 0.1}
{'loss': 0.0111, 'grad_norm': 0.2740453779697418, 'learning_rate': 1.9593346528007217e-05, 'epoch': 0.2}
{'loss': 0.0083, 'grad_norm': 0.3645798861980438, 'learning_rate': 1.7189254929478722e-05, 'epoch': 0.3}
{'loss': 0.0081, 'grad_norm': 0.26770928502082825, 'learning_rate': 1.478516333095023e-05, 'epoch': 0.4}
{'loss': 0.0075, 'grad_norm': 0.25869056582450867, 'learning_rate': 1.2381071732421739e-05, 'epoch': 0.49}
{'loss': 0.0064, 'grad_norm': 0.22058109939098358, 'learning_rate': 9.976980133893246e-06, 'epoch': 0.59}
{'loss': 0.0069, 'grad_norm': 0.6053346395492554, 'learning_rate': 7.572888535364753e-06, 'epoch': 0.69}
{'loss': 0.0064, 'grad_norm': 0.5089040994644165, 'learning_rate': 5.16879693683626e-06, 'epoch': 0.79}
{'loss': 0.0057, 'grad_norm': 0.242877796292305, 'learning_rate': 2.7647053383077665e-06, 'epoch': 0.89}
{'loss': 0.0063, 'grad_norm': 0.30184823274612427, 'learning_rate': 3.606137397792739e-07, 'epoch': 0.99}
{'train_runtime': 502.8914, 'train_samples_per_second': 12.868, 'train_steps_per_second': 0.805, 'train_loss': 0.008131210133433342, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [08:22<00:00,  1.24s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:27<00:00,  3.31it/s]
{'eval_loss': 0.006507176905870438, 'eval_sts-dev_recall@5': 0.19964726631393295, 'eval_sts-dev_recall@10': 0.24052819608375164, 'eval_sts-dev_recall@20': 0.2772938995161217, 'eval_sts-dev_normalized_recall@5': 0.3111111111111111, 'eval_sts-dev_normalized_recall@10': 0.3111111111111111, 'eval_sts-dev_normalized_recall@20': 0.2916971916971917, 'eval_sts-dev_cluster_recall': 0.3333333333333333, 'eval_runtime': 27.2264, 'eval_samples_per_second': 26.445, 'eval_steps_per_second': 3.306}
[I 2026-01-02 15:36:46,227] Trial 17 finished with value: 0.3111111111111111 and parameters: {'learning_rate': 2.0014062557749703e-05, 'weight_decay': 0.12710805238692618, 'warmup_ratio': 0.1757410422996754, 'max_grad_norm': 1.3623660739881227}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0148, 'grad_norm': 0.3567555248737335, 'learning_rate': 1.4583244578492009e-05, 'epoch': 0.1}
{'loss': 0.0104, 'grad_norm': 0.2646740674972534, 'learning_rate': 2.6803525986944145e-05, 'epoch': 0.2}
{'loss': 0.0077, 'grad_norm': 0.2720078229904175, 'learning_rate': 2.3514749792227074e-05, 'epoch': 0.3}
{'loss': 0.0077, 'grad_norm': 0.29820477962493896, 'learning_rate': 2.022597359751e-05, 'epoch': 0.4}
{'loss': 0.0068, 'grad_norm': 0.18475058674812317, 'learning_rate': 1.6937197402792928e-05, 'epoch': 0.49}
{'loss': 0.0057, 'grad_norm': 0.19840718805789948, 'learning_rate': 1.3648421208075853e-05, 'epoch': 0.59}
{'loss': 0.0064, 'grad_norm': 0.4891112744808197, 'learning_rate': 1.035964501335878e-05, 'epoch': 0.69}
{'loss': 0.0059, 'grad_norm': 0.8588720560073853, 'learning_rate': 7.070868818641707e-06, 'epoch': 0.79}
{'loss': 0.0056, 'grad_norm': 0.33551448583602905, 'learning_rate': 3.782092623924634e-06, 'epoch': 0.89}
{'loss': 0.006, 'grad_norm': 0.26184603571891785, 'learning_rate': 4.93316429207561e-07, 'epoch': 0.99}
{'train_runtime': 1004.2877, 'train_samples_per_second': 6.443, 'train_steps_per_second': 0.403, 'train_loss': 0.007645273245411154, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [16:44<00:00,  2.48s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:28<00:00,  3.19it/s]
{'eval_loss': 0.006268060300499201, 'eval_sts-dev_recall@5': 0.16980509202731425, 'eval_sts-dev_recall@10': 0.21450730339619228, 'eval_sts-dev_recall@20': 0.3884050106272328, 'eval_sts-dev_normalized_recall@5': 0.24444444444444446, 'eval_sts-dev_normalized_recall@10': 0.2666666666666667, 'eval_sts-dev_normalized_recall@20': 0.4028083028083028, 'eval_sts-dev_cluster_recall': 0.2222222222222222, 'eval_runtime': 28.3102, 'eval_samples_per_second': 25.432, 'eval_steps_per_second': 3.179}
[I 2026-01-02 15:53:59,830] Trial 18 finished with value: 0.2666666666666667 and parameters: {'learning_rate': 2.7296842416151706e-05, 'weight_decay': 0.12039855800222113, 'warmup_ratio': 0.17793556843589411, 'max_grad_norm': 0.653631649087711}. Best is trial 4 with value: 0.33333333333333326.
Some weights of BertModel were not initialized from the model checkpoint at models\jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.0153, 'grad_norm': 0.34848859906196594, 'learning_rate': 9.867848648436112e-06, 'epoch': 0.1}
{'loss': 0.0113, 'grad_norm': 0.3112494647502899, 'learning_rate': 1.84407762642244e-05, 'epoch': 0.2}
{'loss': 0.0082, 'grad_norm': 0.2176060974597931, 'learning_rate': 1.617810432996374e-05, 'epoch': 0.3}
{'loss': 0.0083, 'grad_norm': 0.2936934530735016, 'learning_rate': 1.3915432395703076e-05, 'epoch': 0.4}
{'loss': 0.0075, 'grad_norm': 0.3768332898616791, 'learning_rate': 1.1652760461442413e-05, 'epoch': 0.49}
{'loss': 0.0065, 'grad_norm': 0.23087243735790253, 'learning_rate': 9.39008852718175e-06, 'epoch': 0.59}
{'loss': 0.007, 'grad_norm': 0.5456191897392273, 'learning_rate': 7.127416592921087e-06, 'epoch': 0.69}
{'loss': 0.0065, 'grad_norm': 0.4179396331310272, 'learning_rate': 4.864744658660424e-06, 'epoch': 0.79}
{'loss': 0.0057, 'grad_norm': 0.23900631070137024, 'learning_rate': 2.602072724399762e-06, 'epoch': 0.89}
{'loss': 0.0065, 'grad_norm': 0.3937118351459503, 'learning_rate': 3.394007901390994e-07, 'epoch': 0.99}
{'train_runtime': 614.1645, 'train_samples_per_second': 10.536, 'train_steps_per_second': 0.659, 'train_loss': 0.008229737107952436, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 405/405 [10:14<00:00,  1.52s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:25<00:00,  3.57it/s]
{'eval_loss': 0.006311573088169098, 'eval_sts-dev_recall@5': 0.19232125898792565, 'eval_sts-dev_recall@10': 0.23290824401935512, 'eval_sts-dev_recall@20': 0.30293492515714737, 'eval_sts-dev_normalized_recall@5': 0.28888888888888886, 'eval_sts-dev_normalized_recall@10': 0.2888888888888889, 'eval_sts-dev_normalized_recall@20': 0.3173382173382173, 'eval_sts-dev_cluster_recall': 0.3333333333333333, 'eval_runtime': 25.2609, 'eval_samples_per_second': 28.502, 'eval_steps_per_second': 3.563}
[I 2026-01-02 16:04:40,359] Trial 19 finished with value: 0.2888888888888889 and parameters: {'learning_rate': 1.8723610256006983e-05, 'weight_decay': 0.04969689087684419, 'warmup_ratio': 0.18153712049524365, 'max_grad_norm': 1.1282424221502554}. Best is trial 4 with value: 0.33333333333333326.
BestRun(run_id='4', objective=0.33333333333333326, hyperparameters={'learning_rate': 1.2247359733257542e-05, 'weight_decay': 0.09092585204374326, 'warmup_ratio': 0.05503071687326718, 'max_grad_norm': 0.8774817671930895}, run_summary=None)

(.venv) E:\ProjectsSSD\School\IS\Idk\SkupnoAng>