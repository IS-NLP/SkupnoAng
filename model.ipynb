{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1335226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\timna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_data_path = \"./data/English dataset/train.jsonl\"\n",
    "test_data_path = \"./data/English dataset/test.jsonl\"\n",
    "\n",
    "def preprocess_text(text): # From the labs\n",
    "\t# Tokenize the text into words\n",
    "\twords = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "\t# Remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\twords = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\twords = [word for word in words if word not in stop_words]\n",
    "\n",
    "\t# Lemmatization\n",
    "\tlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "\t# Join the words back into a string\n",
    "\tpreprocessed_text = ' '.join(lemmatized_words)\n",
    "\treturn preprocessed_text\n",
    "\n",
    "train_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=train_data_path)[\"train\"])\n",
    "test_data = pd.DataFrame(datasets.load_dataset(\"json\", data_files=test_data_path)[\"train\"])\n",
    "\n",
    "label_map = {\"Contradiction\": 1, \"Entailment\": 0, \"NotMentioned\": 0}\n",
    "train_data[\"label\"] = train_data[\"label\"].map(label_map)\n",
    "test_data[\"label\"] = test_data[\"label\"].map(label_map)\n",
    "\n",
    "train_data = train_data.drop(\"doc_id\", axis=1)\n",
    "train_data = train_data.drop(\"key\", axis=1)\n",
    "test_data = test_data.drop(\"doc_id\", axis=1)\n",
    "test_data = test_data.drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f978cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "#model_name = \"kiddothe2b/longformer-mini-1024\"\n",
    "model_name = \"./trained_model_ex3_f1_class1_weighted\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348ab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "\tt =  tokenizer(examples[\"premise\"], examples[\"hypothesis\"], max_length=1024, truncation=\"only_first\", padding=\"max_length\")\n",
    "\tt[\"labels\"] = examples[\"label\"]\n",
    "\treturn t\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize as usual\n",
    "    inputs = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], \n",
    "                       max_length=1024, truncation=\"only_first\", padding=\"max_length\")\n",
    "    \n",
    "    # Initialize mask with 0s (Local Attention)\n",
    "    global_attention_mask = [[0] * len(ids) for ids in inputs[\"input_ids\"]]\n",
    "    \n",
    "    # Set the first token (index 0) to 1 (Global Attention)\n",
    "    for mask in global_attention_mask:\n",
    "        mask[0] = 1 \n",
    "        \n",
    "    inputs[\"global_attention_mask\"] = global_attention_mask\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20206744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7191/7191 [00:10<00:00, 706.79 examples/s]\n",
      "Map: 100%|██████████| 2091/2091 [00:02<00:00, 699.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(test_data)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48006a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision_c1 = precision_score(labels, predictions, pos_label=1, average='binary')\n",
    "    \n",
    "    return {\"precision_class_1\": precision_c1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22741f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 2:23:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./artifacts\",\n",
    "\tlearning_rate=2e-4,\n",
    "\tper_device_train_batch_size=2,\n",
    "\tgradient_accumulation_steps=16,\n",
    "\tnum_train_epochs=1,\n",
    "\tweight_decay=0.01,\n",
    "\tsave_strategy=\"steps\",\n",
    "\tsave_steps=50, \n",
    "\tsave_total_limit=3, \n",
    "\n",
    "    metric_for_best_model=\"precision_class_1\", \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=tokenized_dataset,\n",
    "\tcompute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "tokenizer.save_pretrained(\"./trained_model_ex3\")\n",
    "trainer.save_model(\"./trained_model_ex3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c28571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 2:38:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336579</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336391</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Super bad\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\tlogits, labels = eval_pred\n",
    "\t\n",
    "\tpredictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "\tf1_score_class_1 = f1_score(labels, predictions, pos_label=1, average='binary')\n",
    "\n",
    "\treturn {\"f1_score_class_1\": f1_score_class_1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./artifacts\",\n",
    "\tlearning_rate=2e-4,\n",
    "\tper_device_train_batch_size=2,\n",
    "\tgradient_accumulation_steps=16,\n",
    "\tnum_train_epochs=1,\n",
    "\tweight_decay=0.01,\n",
    "\tsave_strategy=\"steps\",\n",
    "\tsave_steps=100,\n",
    "\tsave_total_limit=3, \n",
    "\tload_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_score_class_1\", \n",
    "    \n",
    "\teval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "\tgreater_is_better=True\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=tokenized_dataset,\n",
    "\tcompute_metrics=compute_metrics,\n",
    "\teval_dataset = tokenized_eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "tokenizer.save_pretrained(\"./trained_model_ex3\")\n",
    "trainer.save_model(\"./trained_model_ex3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "705c1dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 3:25:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285482</td>\n",
       "      <td>0.817582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313472</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.233095</td>\n",
       "      <td>0.803456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.210810</td>\n",
       "      <td>0.809322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.811456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# --- 1. Define the Weighted Trainer ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Move weights to the correct device (GPU/CPU)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            # Fallback to default loss if no weights are provided\n",
    "            loss = outputs.loss if isinstance(outputs, dict) else outputs[0]\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 2. Metrics with 0.3 Threshold ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    f1 = f1_score(labels, predictions, pos_label=1, average='binary')\n",
    "    return {\"f1_score_class_1\": f1}\n",
    "\n",
    "# --- 3. Configuration ---\n",
    "# Since you have a 9:1 ratio:\n",
    "# Weight for Class 0 = 1.0\n",
    "# Weight for Class 1 = 9.0\n",
    "class_weights = [1.0, 9.0]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./artifacts\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_score_class_1\", \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    greater_is_better=True,\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "# --- 4. Execution ---\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save results\n",
    "tokenizer.save_pretrained(\"./trained_model_ex3\")\n",
    "trainer.save_model(\"./trained_model_ex3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14a38801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timna\\Desktop\\ISsem2_Cmpy\\SkupnoAng\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS PERFORMANCE\n",
      "┏━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
      "┃    ┃  precision  ┃  recall  ┃  f1-score  ┃  support  ┃\n",
      "┣━━━━╋━━━━━━━━━━━━━╋━━━━━━━━━━╋━━━━━━━━━━━━╋━━━━━━━━━━━┫\n",
      "┃ 0  ┃    0.982    ┃  0.974   ┃   0.978    ┃   1871    ┃\n",
      "┃ 1  ┃    0.791    ┃  0.845   ┃   0.818    ┃    220    ┃\n",
      "┗━━━━┻━━━━━━━━━━━━━┻━━━━━━━━━━┻━━━━━━━━━━━━┻━━━━━━━━━━━┛\n",
      "\n",
      "GLOBAL AVERAGES\n",
      "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
      "┃              ┃  precision  ┃  recall  ┃  f1-score  ┃\n",
      "┣━━━━━━━━━━━━━━╋━━━━━━━━━━━━━╋━━━━━━━━━━╋━━━━━━━━━━━━┫\n",
      "┃ accuracy     ┃    0.96     ┃   0.96   ┃    0.96    ┃\n",
      "┃ macro avg    ┃    0.887    ┃   0.91   ┃   0.898    ┃\n",
      "┃ weighted avg ┃    0.962    ┃   0.96   ┃   0.961    ┃\n",
      "┗━━━━━━━━━━━━━━┻━━━━━━━━━━━━━┻━━━━━━━━━━┻━━━━━━━━━━━━┛\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"trained_model_ex3\", num_labels=2)\n",
    "\n",
    "def pretty_print_report_dict(report):\n",
    "\treport_df = pd.DataFrame(report).transpose()\n",
    "\treport_df = report_df.round(3)\n",
    "\n",
    "\tclass_metrics = report_df.iloc[:-3, :].copy()\n",
    "\n",
    "\tsummary_metrics = report_df.iloc[-3:, :].copy()\n",
    "\tsummary_metrics = summary_metrics.drop(columns=['support'])\n",
    "\n",
    "\tprint(\"CLASS PERFORMANCE\")\n",
    "\tprint(tabulate(class_metrics, headers='keys', tablefmt='heavy_outline', numalign=\"center\"))\n",
    "\tprint()\n",
    "\tprint(\"GLOBAL AVERAGES\")\n",
    "\tprint(tabulate(summary_metrics, headers='keys', tablefmt='heavy_outline', numalign=\"center\"))\n",
    "\n",
    "trainer = Trainer(model=model)  # no need for args for evaluation \n",
    "predictions_procentages = trainer.predict(tokenized_eval_dataset)[0]\n",
    "predictions = predictions_procentages.argmax(-1)\n",
    "\n",
    "#probs = torch.nn.functional.softmax(torch.from_numpy(predictions_procentages.predictions), dim=-1).numpy()\n",
    "#threshold = 0.4\n",
    "#predictions = (probs[:, 1] >= threshold).astype(int)\n",
    "\n",
    "report_dict = classification_report(test_data[\"label\"], predictions, zero_division=0, output_dict=True)\n",
    "pretty_print_report_dict(report_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
